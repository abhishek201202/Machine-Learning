{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) MLP (multi layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(20,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes = (20,) , max_iter = 2000 )\n",
    "## We can't use different activation function for different hidden layers in MLPClassifier\n",
    "## y_train is not one hot encoded but still algorithm will take care of it\n",
    "clf.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 4.21437183e-01, -2.43333585e-01, -1.65301206e-01,\n",
       "          1.05311109e-01,  3.91025514e-01, -1.03884267e-01,\n",
       "         -2.99333080e-02, -8.51611747e-03,  7.20377915e-02,\n",
       "          6.01646091e-01, -4.83492645e-04,  3.47012095e-01,\n",
       "         -5.20502043e-02, -6.06912084e-02,  5.55551833e-01,\n",
       "         -3.81439453e-03,  4.95750337e-01,  2.76454649e-01,\n",
       "         -2.67923644e-01, -5.91872528e-22],\n",
       "        [-5.78311661e-01,  2.82053208e-03,  5.20548540e-01,\n",
       "          7.23018191e-01, -3.99872448e-01, -1.81662417e-02,\n",
       "         -1.94602241e-02, -4.78672496e-06,  4.93081804e-02,\n",
       "          6.79055461e-01, -2.50302556e-03, -3.65661326e-01,\n",
       "          7.00150289e-01, -7.51968638e-01,  6.18523015e-01,\n",
       "         -6.68393052e-08, -3.69926364e-01, -4.93167715e-01,\n",
       "          1.20227090e-01, -1.51472542e-18],\n",
       "        [ 8.30725790e-01,  1.48418748e-01, -2.06968781e-02,\n",
       "          3.12101290e-01,  4.68794215e-01, -5.59762908e-02,\n",
       "         -7.40720009e-02,  2.73289273e-04,  3.13379868e-02,\n",
       "         -8.99823725e-01,  3.73965291e-15, -4.03285491e-02,\n",
       "         -1.58297199e-01,  4.62623756e-01, -8.20892748e-01,\n",
       "          3.09523388e-10,  1.10065090e-01,  1.13357928e-02,\n",
       "          7.84911060e-01, -1.22568571e-06],\n",
       "        [ 2.46336759e-01,  4.02634071e-01, -7.13951253e-01,\n",
       "         -8.12944797e-02,  2.59098432e-01,  2.51793940e-02,\n",
       "         -8.94217566e-03, -6.48693510e-04,  1.46121197e-02,\n",
       "         -9.59437398e-01, -4.46351632e-07, -2.88274683e-01,\n",
       "         -2.13433724e-02,  7.08528213e-01, -9.01691939e-01,\n",
       "         -2.07702650e-03, -2.88341647e-01, -2.98728073e-01,\n",
       "          3.47474343e-01, -5.10323844e-04]]),\n",
       " array([[-6.44108687e-01,  1.43804220e-01,  1.04963341e+00],\n",
       "        [-1.06689524e-01,  2.85489966e-01, -3.77729223e-01],\n",
       "        [ 4.99744680e-01, -6.46154485e-01,  9.11732906e-02],\n",
       "        [ 2.36048909e-01, -6.85095829e-02, -4.95026083e-01],\n",
       "        [-3.08481473e-01,  7.81734578e-02,  1.37875475e-01],\n",
       "        [ 5.11988602e-07,  5.68087944e-03,  6.59590801e-05],\n",
       "        [ 4.11288554e-11,  5.64052596e-03, -9.36978434e-13],\n",
       "        [ 1.97379103e-07,  3.67550555e-03,  4.92577429e-03],\n",
       "        [ 4.05973922e-01, -3.75527717e-01,  5.12482232e-02],\n",
       "        [ 9.61513915e-01,  7.42945207e-01, -9.15547086e-01],\n",
       "        [ 4.18253651e-27, -1.31985834e-15,  6.15109423e-20],\n",
       "        [-2.62277935e-01,  1.17338756e-01,  8.29072762e-02],\n",
       "        [ 2.71067931e-01, -4.10265055e-01, -2.30633409e-01],\n",
       "        [ 2.31996471e-01, -4.70596461e-01,  7.07441287e-01],\n",
       "        [ 1.06276094e+00,  7.73253996e-01, -9.91940588e-01],\n",
       "        [-2.40025167e-05,  2.43486052e-03,  1.25735917e-03],\n",
       "        [-7.10749588e-02,  5.64898657e-01, -4.67488210e-01],\n",
       "        [ 3.03780957e-01, -2.50077209e-02, -9.63974847e-02],\n",
       "        [-8.41455589e-01,  1.34815253e-01,  8.97668238e-01],\n",
       "        [-8.21823653e-03,  1.52726306e-03, -4.67495387e-06]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(clf.coefs_))\n",
    "clf.coefs_\n",
    "## this not include the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 20), (20, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_[0].shape , clf.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20,), (3,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## intercepts are here represent as biases\n",
    "clf.intercepts_[0].shape , clf.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Forward Propogation && Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) No - Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0 , 0] , [0 , 1] , [1 , 0] , [1 , 1]])\n",
    "y = np.array([[0 , 0 , 0 , 1]]).T\n",
    "x.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z) * (1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.33432302],\n",
       "        [-0.04496778]]), array([0.97089544]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no hidden layer weights \n",
    "weights = 2 * np.random.random((2 , 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "lr = 0.1\n",
    "## (2 , 1) && 1 is to select the shape of random no. which we want\n",
    "## multiply by 2 and then subtract it from 1 will basically change the range from -1 to 1\n",
    "weights , bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.75176914],\n",
       "        [6.75176928]]), array([-11.68835591]), array([[8.39088720e-06],\n",
       "        [7.12788984e-03],\n",
       "        [7.12788887e-03],\n",
       "        [8.59987062e-01]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(3000):\n",
    "    ### forward propogation without any hidden layer\n",
    "    output0 = x\n",
    "    output = sig(np.dot(output0 , weights) + bias)\n",
    "    # print(output)\n",
    "\n",
    "    ### Backward propagation without any hidden layer\n",
    "    first_term = output - y\n",
    "    input_for_last_layer = np.dot(output0 , weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "    first_two = first_term + second_term\n",
    "    # print(first_term.shape , second_term.shape , first_two.shape)\n",
    "\n",
    "    ### update weights and bias\n",
    "    changes = np.array([[0.0] , [0.0]])\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            changes[i][0] += first_two[j][0] * output0[j][i]\n",
    "    weights = weights - lr * changes\n",
    "\n",
    "    bias_change = 0.0\n",
    "    for j in range(4):\n",
    "        bias_change += first_two[j][0] * 1\n",
    "    bias = bias - lr * bias_change\n",
    "    \n",
    "output = sig(np.dot(x , weights) + bias)\n",
    "weights , bias , output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7.86464194],\n",
       "        [7.86464195]]), array([-13.51296413]), array([[1.35329867e-06],\n",
       "        [3.51105255e-03],\n",
       "        [3.51105253e-03],\n",
       "        [9.01705488e-01]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a little optimisation in the above code using vector multiplication\n",
    "for iter in range(3000):\n",
    "    ### forward propogation without any hidden layer\n",
    "    output0 = x\n",
    "    output = sig(np.dot(output0 , weights) + bias)\n",
    "    ### Backward propagation without any hidden layer\n",
    "    first_term = output - y\n",
    "    input_for_last_layer = np.dot(output0 , weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "    first_two = first_term + second_term\n",
    "    ### update weights and bias\n",
    "    changes = np.dot(output0.T , first_two)\n",
    "    weights = weights - lr * changes\n",
    "    bias_change = np.sum(first_two)\n",
    "    bias = bias - lr * bias_change\n",
    "    \n",
    "output = sig(np.dot(x , weights) + bias)\n",
    "weights , bias , output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Hidden Layer is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0 , 0] , [0 , 1] , [1 , 0] , [1 , 1]])\n",
    "y = np.array([[0 , 0 , 0 , 1]]).T\n",
    "print(x.shape , y.shape)\n",
    "\n",
    "wh = 2 * np.random.random((2 , 2)) - 1\n",
    "bh = 2 * np.random.random((1 , 2)) - 1\n",
    "wo = 2 * np.random.random((2 , 1)) - 1\n",
    "bo = 2 * np.random.random((1 , 1)) - 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.13401114],\n",
       "        [0.29650527],\n",
       "        [0.23948181],\n",
       "        [0.42544316]]), array([[-1.45755448, -0.10165064],\n",
       "        [-0.83949521, -1.93504426]]), array([[0.56890557, 0.12851258]]), array([[-1.9337132 ],\n",
       "        [-1.54489806]]), array([[0.19076282]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    # forward propagation with one hidden layer\n",
    "    output0 = x\n",
    "    inputHidden = np.dot(output0 , wh) + bh\n",
    "    outputHidden = sig(inputHidden)\n",
    "    # print(np.dot((output0 , wh)).shape)\n",
    "    # print(temp.shape)\n",
    "    # print(bh.shape)\n",
    "    inputForOutputLayer = np.dot(outputHidden , wo) + bo\n",
    "    output = sig(inputForOutputLayer)\n",
    "    # output\n",
    "\n",
    "\n",
    "\n",
    "    # backward propagation with one hidden layer\n",
    "    first_term_output_layer = output - y\n",
    "    second_term_output_layer = derivativeSig(inputForOutputLayer)\n",
    "    first_two_output_layer = first_term_output_layer * second_term_output_layer\n",
    "\n",
    "\n",
    "    first_term_hidden_layer = np.dot(first_two_output_layer , wo.T)\n",
    "    second_term_hidden_layer = derivativeSig(inputHidden)\n",
    "    first_two_hidden_layer = first_term_hidden_layer * second_term_hidden_layer\n",
    "\n",
    "\n",
    "    changes_output = np.dot(outputHidden.T , first_two_output_layer)\n",
    "    changes_output_bias = np.sum(first_two_output_layer , keepdims = True , axis = 0)\n",
    "\n",
    "    changes_hidden = np.dot(output0.T , first_two_hidden_layer)\n",
    "    changes_hidden_bias = np.sum(first_two_hidden_layer , keepdims = True , axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "    wo = wo - lr * changes_output\n",
    "    bo = bo - lr * changes_output_bias\n",
    "\n",
    "    wh = wh - lr * changes_hidden\n",
    "    bh = bh - lr * changes_hidden_bias\n",
    "    \n",
    "    \n",
    "output0 = x\n",
    "inputHidden = np.dot(output0 , wh) + bh\n",
    "outputHidden = sig(inputHidden)\n",
    "inputForOutputLayer = np.dot(outputHidden , wo) + bo\n",
    "output = sig(inputForOutputLayer)\n",
    "output , wh , bh , wo , bo\n",
    "## see the below output and figure out the function which it learn ( like and , or , nor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "[[1 1]] \n",
      "\n",
      "[[1 1]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "## some calculation\n",
    "x = np.array([[0 , 0] , [0 , 1] , [1 , 0] , [1 , 1]])\n",
    "y = np.array([[1 , 1]])\n",
    "x , y\n",
    "print(x , '\\n')\n",
    "print(y , '\\n')\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
